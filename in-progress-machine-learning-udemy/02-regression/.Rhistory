usedcars <- read.csv("usedcars.csv", stringsAsFactors = FALSE)
View(usedcars)
str(usedcars)
summary(usedcars)
mean(usedcars$price)
median(usedcars$price)
mode(usedcars$price)
min(usedcars$price)
max(usedcars$price)
range(usedcars$price)
diff(range(usedcars$price))
IRQ(usedcars$price)   # returns the difference between Q1 and Q3 - interquartile range
quantile(usedcars$price) # returns the five-number summary
quantile(usedcars$price, probs = c(0.01, 0.99)) # returns 1st and 99th percentiles
IQR(usedcars$price)
quantile(usedcars$price, seq(from = 0, to = 1, by = 0.20))
boxplot(usedcars$price, main="Boxplot of Used Car Prices", ylab="Price ($)")
hist(usedcars$price, main="Histogram of Used Car Prices", xlab="Price ($)")
# visualizing numeric variables - boxplots
# visualize quartiles, min/max and outliers
boxplot(usedcars$price, main="Boxplot of Used Car Prices", ylab="Price ($)")
var(usedcars$price)
sd(usedcars$price)
table(usedcars$year)
prop.table(usedcars$year) # returns the categories and proportions for each
model_table <- table(usedcars$year)
prop.table(model_table)
table_pct <- prop.table(model_table) * 100
round(table_pct, digits = 1)
plot(x = usedcars$milage, y = usedcars$price,
main = "Scatterplot of Price vs. Mileage",
xlab = "Used Car Odometer (mi.)",
ylab = "Used Car Price ($)")
plot(x = usedcars$mileage, y = usedcars$price,
main = "Scatterplot of Price vs. Mileage",
xlab = "Used Car Odometer (mi.)",
ylab = "Used Car Price ($)")
install.packages("gmodels")
library(gmodels)
usedcars$conservative <- usedcars$color %in% c("Black", "Gray", "Silver", "White
usedcars$conservative <- usedcars$color %in% c("Black", "Gray", "Silver", "White")
usedcars$conservative <- NULL
usedcars$conservative <- usedcars$color %in% c("Black", "Gray", "Silver", "White")
CrossTable(x = usedcars$model, y = usedcars$conservative)
CrossTable(x = usedcars$model, y = usedcars$conservative, chisq = TRUE)
wbcd <- read.csv("3-wisc_bc_data.csv", stringsAsFactors = FALSE)
str(wbcd)
wbcd <- wbcd[-1]   # drop id
table(wbcd$diagnosis)
wbcd <- factor(wbcd$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant"))
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)   # return values in % and rounded
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
fart <- as.list(factor(wbcd$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant")))
# load data
wbcd <- read.csv("3-wisc_bc_data.csv", stringsAsFactors = FALSE)
str(wbcd)   # summary info
wbcd <- wbcd[-1]   # drop id
table(wbcd$diagnosis)   # returns the values and counts in feature
source('~/data-science/machine-learning/r/3-ClassificationUsingkNN.R', echo=TRUE)
# load data
wbcd <- read.csv("3-wisc_bc_data.csv", stringsAsFactors = FALSE)
str(wbcd)   # summary info
wbcd <- wbcd[-1]   # drop id
table(wbcd$diagnosis)   # returns the values and counts in feature
fart <- as.list(factor(wbcd$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant")))
wbcd <- as.list(factor(wbcd$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant")))
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
# load data
wbcd <- read.csv("3-wisc_bc_data.csv", stringsAsFactors = FALSE)
str(wbcd)   # summary info
wbcd <- wbcd[-1]   # drop id
table(wbcd$diagnosis)   # returns the values and counts in feature
wbcd <- read.csv("3-wisc_bc_data.csv", stringsAsFactors = FALSE)
# examine the structure of the wbcd data frame
str(wbcd)
# drop the id feature
wbcd <- wbcd[-1]
# table of diagnosis
table(wbcd$diagnosis)
# recode diagnosis as a factor
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant"))
# table or proportions with more informative labels
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
wbcd <- read.csv("3-wisc_bc_data.csv", stringsAsFactors = FALSE)
str(wbcd)   # summary info
wbcd <- wbcd[-1]   # drop id
table(wbcd$diagnosis)   # returns the values and counts in feature
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant"))   # B -> Benign, M -> Malignant
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)   # return values in % and rounded
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
# normalization function takes a vector x and returns a vector of normalized values
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
normalize(c(1, 2, 3, 4, 5))
wbcd_n <- as.data.frame(lapply(wbcd[2:32], normalize))
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
# split data into train+test sets
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
# usually would randomized selection if not already in random order
# split labels into train+test sets
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
install.packages("class")
library(class)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
library(gmodels)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq = FALSE)
summary(wbcd_z$area_mean)
wbcd_z <- as.data.frame(scale(wbcd[-1]))
summary(wbcd_z$area_mean)
wbcd_z_train <- wbcd_z[1:469, ]
wbcd_z_test <- wbcd_z[470:569, ]
source('~/data-science/machine-learning/r/3-ClassificationUsingkNN.R', echo=TRUE)
wbcd_z_test_pred <- knn(train = wbcd_z_train, test = wbcd_z_test, cl = wbcd_train_labels, k=21)
source('~/data-science/machine-learning/r/3-ClassificationUsingkNN.R', echo=TRUE)
sms_raw_train <- sms_raw[1:4169, ]   # 75% testing
sms_raw_test <- sms_raw[4170:5559, ]   # 25% training
# libraries
library(tm)
## Exploring/preparing the data ##
##################################
sms_raw <- read.csv("4-sms_spam.csv", stringsAsFactors = FALSE)
str(sms_raw)
# type is curently character vector but because it's categorical would be better as a factor
sms_raw$type <- factor(sms_raw$type)   # ham -> 1, spam -> 2
str(sms_raw$type)
table(sms_raw$type) # returns table with frequency counts for each value
# Processing text data for analysis #
#####################################
# create corpus - collection of text documents
# Corpus() - creates an R object to sotre text documents
# VectorSource() - tells Corpus() to use messages in given R vector
sms_corpus <- Corpus(VectorSource(sms_raw$text))
print(sms_corpus)   # returns datatype, # of text documents, etc
inspect(sms_corpus[1:3])   # supposed to return texts...
# tm_map() will help transform/map tm corpus
corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))   # convert to lowercase, not a canonical transformation
corpus_clean <- tm_map(corpus_clean, removeNumbers)   # remove numbers
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())   # remove stop/filler words
corpus_clean <- tm_map(corpus_clean, removePunctuation)   # remove punctuation
corpus_clean <- tm_map(corpus_clean, stripWhitespace)   # remove extra whitespace
# tokenization - split messages into individual components
# DocumentTermMatrix() - takes a corpus and creates sparse matric - rows indicate documents, columns words and values counts
sms_dtm <- DocumentTermMatrix(corpus_clean)
# Training and testing datasets #
#################################
sms_raw_train <- sms_raw[1:4169, ]   # 75% testing
sms_raw_test <- sms_raw[4170:5559, ]   # 25% training
sms_dtm_train <- sms_dtm[1:4169, ]   # same proportion for document-term matrix
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_corpus_train <- corpus_clean[1:4169, ]   # same proportion for corpus
sms_corpus_test <- corpus_clean[4170:5559, ]
sms_corpus_train <- corpus_clean[1:4169]   # same proportion for corpus
sms_corpus_test <- corpus_clean[4170:5559]
prop.table(table(sms_raw_train$type))
prop.table(table(sms_raw_test$type))
install.packages("wordcloud")
library(wordcloud)
wordcloud(sms_corpus_train, min.freq = 40, random.order = FALSE)
warnings()
source('~/data-science/machine-learning/r/4-ClassificationUsingNaiveBayes.R', echo=TRUE)
spam <- subset(sms_raw_train, type == "spam")
ham <- subset(sms_raw_train, type =="ham")
wordcloud(spam$text, max.words=40, scale = c(3, 0.5))
wordcloud(ham$text, max.words=40, scale = c(3, 0.5))
sms_dict <- Dictionary(findFreqTerms(sms_dtm_train, 5))
library(tm)
sms_dict <- Dictionary(findFreqTerms(sms_dtm_train, 5))
sms_dict <- Terms(findFreqTerms(sms_dtm_train, 5))
sms_dict <- Terms(c(findFreqTerms(sms_dtm_train, 5)))
sms_dict <- c(findFreqTerms(sms_dtm_train, 5))
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary = sms_dict))
sms_test <- DocumentTermMatrix(sms_corpus_test, list(dictionary = sms_dict))
sms_train <- apply(sms_train, MARGIN = 2, convert_counts)
sms_test <- apply(sms_test, MARGIN = 2, convert_counts)
convert_counts <- function(x) {
x <- ifelse(x > 0, 1, 0)   # if x > 0 then 1 else 0
x <- factor(x, levels = c(0, 1), labels = c("No", "Yes"))   # 0 -> No, 1 -> Yes
return(x)
}
sms_train <- apply(sms_train, MARGIN = 2, convert_counts)
sms_test <- apply(sms_test, MARGIN = 2, convert_counts)
install.packages("e1071")
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
sms_test_pred <- predict(sms_classifier, sms_test)
library(gmodels)
CrossTable(sms_test_pred, sms_raw_test$type, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
sms_classifier2 <- naiveBayes(sms_train, sms_raw_train$type, laplace = 1)
sms_test_pred2 <- predict(sms_classifier2, sms_test)
CrossTable(sms_test_pred2, sms_raw_test$type, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
# libraries
library(tm)
library(wordcloud)
library(e1071)   # training naive Bayes
library(gmodels)   # CrossTable()
## Exploring/preparing the data ##
##################################
sms_raw <- read.csv("4-sms_spam.csv", stringsAsFactors = FALSE)
str(sms_raw)
# type is curently character vector but because it's categorical would be better as a factor
sms_raw$type <- factor(sms_raw$type)   # ham -> 1, spam -> 2
str(sms_raw$type)
table(sms_raw$type) # returns table with frequency counts for each value
# Processing text data for analysis #
#####################################
# create corpus - collection of text documents
# Corpus() - creates an R object to sotre text documents
# VectorSource() - tells Corpus() to use messages in given R vector
sms_corpus <- Corpus(VectorSource(sms_raw$text))
print(sms_corpus)   # returns datatype, # of text documents, etc
inspect(sms_corpus[1:3])   # supposed to return texts...
# tm_map() will help transform/map tm corpus
corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))   # convert to lowercase, not a canonical transformation
corpus_clean <- tm_map(corpus_clean, removeNumbers)   # remove numbers
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())   # remove stop/filler words
corpus_clean <- tm_map(corpus_clean, removePunctuation)   # remove punctuation
corpus_clean <- tm_map(corpus_clean, stripWhitespace)   # remove extra whitespace
# tokenization - split messages into individual components
# DocumentTermMatrix() - takes a corpus and creates sparse matric - rows indicate documents, columns words and values counts
sms_dtm <- DocumentTermMatrix(corpus_clean)
# Training and testing datasets #
#################################
sms_raw_train <- sms_raw[1:4169, ]   # 75% testing
sms_raw_test <- sms_raw[4170:5559, ]   # 25% training
sms_dtm_train <- sms_dtm[1:4169, ]   # same proportion for document-term matrix
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_corpus_train <- corpus_clean[1:4169]   # same proportion for corpus
sms_corpus_test <- corpus_clean[4170:5559]
# compare proportion of spam to ham in training and test dataframes to ensure randomness
prop.table(table(sms_raw_train$type))
prop.table(table(sms_raw_test$type))
# Visualizing text data - word clouds #
#######################################
# random.order - place more frequent words in middle of cloud
# min.freq - number of times a word must appear to be placed in cloud, roughly 10% of number of documents in corpus
wordcloud(sms_corpus_train, min.freq = 40, random.order = FALSE)
# spam and ham subsets to view cloud by type
spam <- subset(sms_raw_train, type == "spam")
ham <- subset(sms_raw_train, type =="ham")
# max.words - top X most common words in the dataframe
wordcloud(spam$text, max.words=40, scale = c(3, 0.5))
wordcloud(ham$text, max.words=40, scale = c(3, 0.5))
# lots of variation between clouds, naive bayes will likely work well
# Data preparation - creating indicator features #
##################################################
# too many features in sparse matrix now, so only include terms that appear at least 5 times (0.1%) of records in training dataset
sms_dict <- c(findFreqTerms(sms_dtm_train, 5))
# use the dictionary to specify which words should appear in the document term matrix
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary = sms_dict))
sms_test <- DocumentTermMatrix(sms_corpus_test, list(dictionary = sms_dict))
# naive Bayes needs categorical features (not numeric counts), so change to factor variable
convert_counts <- function(x) {
x <- ifelse(x > 0, 1, 0)   # if x > 0 then 1 else 0
x <- factor(x, levels = c(0, 1), labels = c("No", "Yes"))   # 0 -> No, 1 -> Yes
return(x)
}
# apply convert_counts to train and test datasets
# MARGIN - specifies rows or columns (1 - rows, 2 - columns)
sms_train <- apply(sms_train, MARGIN = 2, convert_counts)
sms_test <- apply(sms_test, MARGIN = 2, convert_counts)
## Training a model ##
######################
# naiveBayes classifier:
# class - factor vector with class for each row
# laplace (optional) - number to control laplace estimator (defaults to 0)
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
## Evaluating model performance ##
##################################
sms_test_pred <- predict(sms_classifier, sms_test)
# use CrossTable to compare predicted to actual values
# use parameters to eliminate unecessary proportions and dnn to relabel rows and columns
CrossTable(sms_test_pred, sms_raw_test$type, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
## Improving model performance ##
#################################
# set laplace = 1 to avoid 0 count for words which sometimes eliminates possibility a word could only appear in spam/ham
sms_classifier2 <- naiveBayes(sms_train, sms_raw_train$type, laplace = 1)
sms_test_pred2 <- predict(sms_classifier2, sms_test)
CrossTable(sms_test_pred2, sms_raw_test$type, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
# although hardly ever changed, likely still better because can generalize
install.packages('rsconnect')
rsconnect::setAccountInfo(name='data-vis-alena', token='36E9A28307BEC9987449D01972DBFD7B', secret='ol9QkrMCzF/+jEiEJkjJ4uUrsO55TGWvMOFDjeXG')
install.packages(c('ggplot2', 'shiny'))
ls
pwd
library(rpart)
## Data Preprocessing ##
# import dataset
dataset = read.csv("Position_Salaries.csv")
# independent variable - level
# dependent variables - salary
# delete position variable
dataset = dataset[2:3]
setwd("~/data-science/Udemy-notes/machine-learning/02-regression")
library(rpart)
## Data Preprocessing ##
# import dataset
dataset = read.csv("Position_Salaries.csv")
# independent variable - level
# dependent variables - salary
# delete position variable
dataset = dataset[2:3]
install.packages('randomForest')
library(randomForest)
View(dataset)
regressor = randomForest(x = dataset[1], y = dataset$Salary, ntree = 10)
# dataset[1] returns a dataset, dataset$Salary returns a vector
summary(regressor)
set.seed(1234)
regressor = randomForest(x = dataset[1], y = dataset$Salary, ntree = 10)
# dataset[1] returns a dataset, dataset$Salary returns a vector
summary(regressor)
library(ggplot2)
# need high resolution because non-continuous
X_grid = seq(min(dataset$Level), max(dataset$Level), 0.01)
ggplot() + geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red') +
geom_line(aes(x = X_grid, y = predict(regressor, newdata = data.frame(Level = X_grid))), color = 'blue') +
ggtitle('Random Forest Regression') +
xlab('Level') +
ylab('Salary')
y_pred = predict(regressor, data.frame(Level = 6.5))
y_pred
regressor = randomForest(x = dataset[1], y = dataset$Salary, ntree = 300)
# dataset[1] returns a dataset, dataset$Salary returns a vector
# increasing trees tends to improve accuracy
summary(regressor)
y_pred = predict(regressor, data.frame(Level = 6.5))
y_pred
regressor = randomForest(x = dataset[1], y = dataset$Salary, ntree = 500)
# dataset[1] returns a dataset, dataset$Salary returns a vector
# increasing trees tends to improve accuracy
summary(regressor)
y_pred = predict(regressor, data.frame(Level = 6.5))
y_pred
